{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Project 3\n",
    "## Author: Brailey Sharpe\n",
    "## Version: Fall 2025"
   ],
   "id": "f95d5872667edd1f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-01T00:04:01.879037Z",
     "start_time": "2025-12-01T00:04:01.873509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ],
   "id": "8954fa1ed3497363",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load and Prepare Tweet Dataset from Multiple Sources\n",
    "\n",
    "The tweet dataset for this project was collected by multiple students in the class. Originally, each CSV file in the `data` folder was named after the individual student who collected it. For privacy reasons, these filenames were renamed to a generic pattern (`stu1.csv`, `stu2.csv`, â€¦, `stu14.csv`) so that no student names appear in this notebook or any public repository.\n",
    "\n",
    "All CSV files in the `data` directory are loaded and combined into a single dataframe. For each file, only rows where `type == \"tweet\"` are kept so that mock records and other non-tweet entries are removed. The combined dataset is then restricted to the 20 common columns specified in Part 1 of the project:\n",
    "\n",
    "- `id`, `url`, `twitterUrl`, `text`, `source`\n",
    "- `retweetCount`, `replyCount`, `likeCount`, `quoteCount`, `viewCount`\n",
    "- `createdAt`, `lang`, `bookmarkCount`\n",
    "- `isReply`, `inReplyToId`, `conversationId`, `inReplyToUsername`\n",
    "- `isPinned`, `isConversationControlled`, `isQuote`\n",
    "\n",
    "Rows that are missing key fields (`id`, `text`, or `createdAt`) are discarded. After concatenating all files, duplicate tweets are removed in two stages: first, duplicates with the same `id` are dropped, and then any remaining rows that are exact duplicates across all 20 columns are removed. To see more information about the dataset, find `README.md`"
   ],
   "id": "a63810cbffac2d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T00:04:02.954810Z",
     "start_time": "2025-12-01T00:04:01.903436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TWITTER_DATETIME_FORMAT = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "\n",
    "COMMON_COLS = [\n",
    "    \"id\", \"url\", \"twitterUrl\", \"text\", \"source\",\n",
    "    \"retweetCount\", \"replyCount\", \"likeCount\", \"quoteCount\", \"viewCount\",\n",
    "    \"createdAt\", \"lang\", \"bookmarkCount\",\n",
    "    \"isReply\", \"inReplyToId\", \"conversationId\", \"inReplyToUsername\",\n",
    "    \"isPinned\", \"isConversationControlled\", \"isQuote\"\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"retweetCount\", \"replyCount\", \"likeCount\",\n",
    "    \"quoteCount\", \"viewCount\", \"bookmarkCount\"\n",
    "]\n",
    "\n",
    "\n",
    "def load_and_clean_single(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and clean a single student CSV file.\"\"\"\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "    if \"type\" in df.columns:\n",
    "        df = df[df[\"type\"] == \"tweet\"].copy()\n",
    "\n",
    "    for col in COMMON_COLS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA\n",
    "\n",
    "    df[\"createdAt\"] = pd.to_datetime(\n",
    "        df[\"createdAt\"],\n",
    "        format=TWITTER_DATETIME_FORMAT,\n",
    "        errors=\"coerce\",\n",
    "        utc=True\n",
    "    )\n",
    "\n",
    "    for col in NUMERIC_COLS:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"id\", \"text\", \"createdAt\"])\n",
    "    df = df[COMMON_COLS].copy()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_all_data(data_folder: str = \"data\") -> pd.DataFrame:\n",
    "    \"\"\"Load, clean, and combine all student CSV files.\"\"\"\n",
    "    pattern = os.path.join(data_folder, \"*.csv\")\n",
    "    file_paths = glob.glob(pattern)\n",
    "\n",
    "    if not file_paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found in folder: {data_folder}\")\n",
    "\n",
    "    frames = []\n",
    "    for fp in file_paths:\n",
    "        print(f\"Loading and cleaning: {fp}\")\n",
    "        cleaned = load_and_clean_single(fp)\n",
    "        frames.append(cleaned)\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    before_id = len(df)\n",
    "    df = df.drop_duplicates(subset=\"id\", keep=\"first\")\n",
    "    after_id = len(df)\n",
    "    print(f\"Removed {before_id - after_id} duplicates by ID.\")\n",
    "\n",
    "    before_full = len(df)\n",
    "    df = df.drop_duplicates(subset=COMMON_COLS, keep=\"first\")\n",
    "    after_full = len(df)\n",
    "    print(f\"Removed {before_full - after_full} full-content duplicates.\")\n",
    "\n",
    "    print(\"Final dataframe shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "df = load_all_data(\"data\")\n"
   ],
   "id": "379ded6d4b5b184f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning: data\\stu1.csv\n",
      "Loading and cleaning: data\\stu10.csv\n",
      "Loading and cleaning: data\\stu11.csv\n",
      "Loading and cleaning: data\\stu12.csv\n",
      "Loading and cleaning: data\\stu13.csv\n",
      "Loading and cleaning: data\\stu14.csv\n",
      "Loading and cleaning: data\\stu2.csv\n",
      "Loading and cleaning: data\\stu3.csv\n",
      "Loading and cleaning: data\\stu4.csv\n",
      "Loading and cleaning: data\\stu5.csv\n",
      "Loading and cleaning: data\\stu6.csv\n",
      "Loading and cleaning: data\\stu7.csv\n",
      "Loading and cleaning: data\\stu8.csv\n",
      "Loading and cleaning: data\\stu9.csv\n",
      "Removed 867 duplicates by ID.\n",
      "Removed 0 full-content duplicates.\n",
      "Final dataframe shape: (20653, 20)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T00:04:02.998746Z",
     "start_time": "2025-12-01T00:04:02.995537Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1903023c584b5a90",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
